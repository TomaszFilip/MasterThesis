{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T01:02:40.085692Z",
     "start_time": "2025-06-12T01:02:39.980409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A,B,G,T=0,0,0,0\n",
    "isColab=False\n",
    "if isColab:\n",
    "    %pip install ultralytics\n",
    "    import ultralytics\n",
    "    ultralytics.checks()\n",
    "    from google.colab.patches import cv2_imshow\n",
    "    from google.colab import drive\n",
    "from sqlalchemy.dialects.oracle.dictionary import all_objects\n",
    "import csv\n",
    "import pickle\n",
    "from models.TrackedObject import TrackedObject\n",
    "from models.VideoAnalysis import VideoAnalysis\n",
    "from utils.dynamics import EuclideanDistTracker, get_overlap_info\n",
    "import utils.files as fil\n",
    "import cv2\n",
    "import os\n",
    "import analysis.mask_rcnn as mask_rcnn\n",
    "from analysis import yolo\n",
    "from utils.visualise import visualize\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import PIL.Image"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "transform, COCO_INSTANCE_CATEGORY_NAMES, model, device= mask_rcnn.config()\n",
    "model= yolo.config()\n",
    "\n",
    "def run_detection(frame_filenames, detection_file_path):\n",
    "    if os.path.exists(detection_file_path):\n",
    "        print(\"Detection file already exists. Skipping YOLO detection.\")\n",
    "        return\n",
    "\n",
    "    print(\"Running YOLO detection and saving results...\")\n",
    "    all_detections = []\n",
    "    i=0\n",
    "    for frame_filename in frame_filenames:\n",
    "        i+=1\n",
    "        if True:# i > 14000:\n",
    "            print(i)\n",
    "            frame = cv2.imread(frame_filename)\n",
    "            if frame is None:\n",
    "                all_detections.append([])  # Append empty detection for consistency\n",
    "                continue\n",
    "\n",
    "            detections = yolo.process_frame(model, frame, device, transform,\n",
    "                                            COCO_INSTANCE_CATEGORY_NAMES,\n",
    "                                            tracker=None,\n",
    "                                            conf=0.5)\n",
    "            all_detections.append(detections)\n",
    "            if i % 1000 == 0:\n",
    "\n",
    "                with open(detection_file_path+str(i), 'wb') as f:\n",
    "                    pickle.dump(all_detections, f)\n",
    "                    print(f\"Saved detections to {detection_file_path+str(i)}\")\n",
    "                    all_detections = []\n",
    "    with open(detection_file_path+str(i), 'wb') as f:\n",
    "        pickle.dump(all_detections, f)\n",
    "        print(f\"Saved detections to {detection_file_path+str(i)}\")\n",
    "        all_detections = []\n",
    "\n",
    "\n",
    "def get_range_id(frame_number, ranges):\n",
    "    for idx, (start, end) in enumerate(ranges):\n",
    "        if start <= frame_number <= end:\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "def process_clip(frame_filenames, case_folder, detection_file_path):\n",
    "    # Each tuple is (start_frame, end_frame), inclusive\n",
    "    accident_ranges = [(2090, 2410),(46822,47023),(51718,51924), (55506, 55746), (58301, 58450)]\n",
    "    counted_ranges = set()\n",
    "    accident_cnt=0\n",
    "    detected_accidents = set()\n",
    "    acc_detected=False\n",
    "    tracker = EuclideanDistTracker()\n",
    "    va=VideoAnalysis()\n",
    "\n",
    "    import glob\n",
    "\n",
    "    # Gather all detection chunks\n",
    "    detection_files = sorted(\n",
    "    glob.glob(detection_file_path + \"*\"),\n",
    "    key=lambda x: int(x.split(detection_file_path)[-1])\n",
    ")\n",
    "    if not detection_files:\n",
    "        raise FileNotFoundError(f\"No detection files found with prefix {detection_file_path}\")\n",
    "\n",
    "    all_detections = []\n",
    "    for file in detection_files:\n",
    "        print(f\"Loading detections from {file}\")\n",
    "        with open(file, 'rb') as f:\n",
    "            detections_chunk = pickle.load(f)\n",
    "            all_detections.extend(detections_chunk)\n",
    "\n",
    "\n",
    "    # Get frame size dynamically from the first frame\n",
    "    sample_frame = cv2.imread(frame_filenames[0])\n",
    "    height, width = sample_frame.shape[:2]\n",
    "    frame_size = (width, height)\n",
    "    fps = 18\n",
    "\n",
    "    # Save output to MP4 instead of AVI\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or use 'avc1' for better compatibility\n",
    "    out = cv2.VideoWriter('D:/f/'+case_folder+str(T)+\"_output_yolo.mp4\", fourcc, fps, frame_size)\n",
    "\n",
    "    for idx, frame_filename in enumerate(frame_filenames):\n",
    "        frame = cv2.imread(frame_filename)\n",
    "        if frame is None:\n",
    "            continue\n",
    "        #result_frame,_,_objects = mask_rcnn.process_frame(model, frame, device, transform, COCO_INSTANCE_CATEGORY_NAMES, tracker, conf=0.5)\n",
    "        _objects = all_detections[idx]\n",
    "        boxes = tracker.update(_objects)\n",
    "        get_overlap_info(boxes)\n",
    "\n",
    "        for o in va.allObjects:\n",
    "          o.is_present=False\n",
    "          o.is_accident=False\n",
    "        for obj in boxes:\n",
    "          if obj.id>(len(va.allObjects)-1):\n",
    "            obj.H=height\n",
    "            va.allObjects.append(obj)\n",
    "          else:\n",
    "            o=va.allObjects[obj.id]\n",
    "            o.id=obj.id\n",
    "            o.is_present=obj.is_present\n",
    "            o.x1=obj.x1\n",
    "            o.x2=obj.x2\n",
    "            o.y1=obj.y1\n",
    "            o.y2=obj.y2\n",
    "            o.centroid=obj.centroid\n",
    "            o.overlaps=obj.overlaps\n",
    "            o.past_centroids.append(obj.centroid)\n",
    "            o.compute_speed()\n",
    "            o.compute_acceleration()\n",
    "            o.compute_vector()\n",
    "            #print(str(o.id)+str(o.is_accident)+str(o.centroid))\n",
    "        #va.allObjects=get_overlap_info(va.allObjects)\n",
    "        for box in va.allObjects:\n",
    "            box.check_accident(A, B, G, T)\n",
    "            if box.is_accident and box.id not in detected_accidents:\n",
    "                range_id = get_range_id(idx, accident_ranges)\n",
    "                if range_id is None:\n",
    "                    accident_cnt += 1\n",
    "                detected_accidents.add(box.id)\n",
    "        result_frame=visualize(frame, va.allObjects)\n",
    "        #print(frame_filename)\n",
    "        out.write(result_frame)\n",
    "        if isColab:\n",
    "            #Convert to PIL format for proper display in Jupyter/Colab\n",
    "            #Display only the current frame\n",
    "            #clear_output(wait=True)\n",
    "            result_frame_rgb = cv2.cvtColor(result_frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            result_pil = PIL.Image.fromarray(result_frame_rgb)\n",
    "            display(result_pil)\n",
    "        else:\n",
    "            cv2.imshow('res',result_frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                  break\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return acc_detected, accident_cnt\n"
   ],
   "id": "62e76456fc0e4c32",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.40  Python-3.12.7 torch-2.5.1+cpu CPU (12th Gen Intel Core(TM) i5-1235U)\n",
      "Setup complete  (12 CPUs, 15.7 GB RAM, 447.1/454.3 GB disk)\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-12T01:02:42.782489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if isColab:\n",
    "    drive.mount('/content/drive/',force_remount=True)\n",
    "    frames = '/content/drive/MyDrive/data/normal/'\n",
    "else:\n",
    "    frames = './data/test/'\n",
    "\n",
    "results=[]\n",
    "A,B,G,T=-7,0,4,0\n",
    "for T in [17,19,21,23,25,27,29,31,33,35]:\n",
    "    folders = os.listdir(frames)\n",
    "    accident_count, total_count=0,0\n",
    "    for case_folder in folders:\n",
    "        frame_filenames= sorted(\n",
    "            [f for f in os.listdir(frames+case_folder) if f.endswith('.jpg')],\n",
    "            key=fil.extract_number\n",
    "        )\n",
    "\n",
    "        frame_filenames = [os.path.join(frames+case_folder, f) for f in frame_filenames]\n",
    "        detection_file_path = case_folder + \"_detections.pkl\"\n",
    "        if False:# not os.path.exists(detection_file_path+\"****\"):\n",
    "            run_detection(frame_filenames, detection_file_path)\n",
    "        else:\n",
    "            print(f\"Detections already exist at {detection_file_path}. Skipping detection.\")\n",
    "        acc_detected, cnt = process_clip(frame_filenames, case_folder, detection_file_path)\n",
    "        if acc_detected == True:\n",
    "            accident_count+=1\n",
    "        total_count+=1\n",
    "    print (\"Total count: \",cnt)\n",
    "    #print (\"Accident count: \",accident_count)\n",
    "    #print (\"A\",A,\"B\",B,\"G\",G)\n",
    "\n",
    "    results.append({\n",
    "        \"A\": A,\n",
    "        \"total_count\": cnt,\n",
    "        \"accident_count\": accident_count,\n",
    "        \"B\": B,\n",
    "        \"G\": G,\n",
    "        \"T\": T\n",
    "    })\n",
    "csv_output_file = \"test_combinedmod.csv\"\n",
    "with open(csv_output_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = [\"A\", \"total_count\", \"accident_count\", \"B\", \"G\",\"T\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)"
   ],
   "id": "8cca68b8194587e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections already exist at combined_detections.pkl. Skipping detection.\n",
      "Loading detections from combined_detections.pkl1000\n",
      "Loading detections from combined_detections.pkl2000\n",
      "Loading detections from combined_detections.pkl3000\n",
      "Loading detections from combined_detections.pkl4000\n",
      "Loading detections from combined_detections.pkl5000\n",
      "Loading detections from combined_detections.pkl6000\n",
      "Loading detections from combined_detections.pkl7000\n",
      "Loading detections from combined_detections.pkl8000\n",
      "Loading detections from combined_detections.pkl9000\n",
      "Loading detections from combined_detections.pkl10000\n",
      "Loading detections from combined_detections.pkl11000\n",
      "Loading detections from combined_detections.pkl12000\n",
      "Loading detections from combined_detections.pkl13000\n",
      "Loading detections from combined_detections.pkl14000\n",
      "Loading detections from combined_detections.pkl15000\n",
      "Loading detections from combined_detections.pkl16000\n",
      "Loading detections from combined_detections.pkl17000\n",
      "Loading detections from combined_detections.pkl18000\n",
      "Loading detections from combined_detections.pkl19000\n",
      "Loading detections from combined_detections.pkl20000\n",
      "Loading detections from combined_detections.pkl21000\n",
      "Loading detections from combined_detections.pkl22000\n",
      "Loading detections from combined_detections.pkl23000\n",
      "Loading detections from combined_detections.pkl24000\n",
      "Loading detections from combined_detections.pkl25000\n",
      "Loading detections from combined_detections.pkl26000\n",
      "Loading detections from combined_detections.pkl27000\n",
      "Loading detections from combined_detections.pkl28000\n",
      "Loading detections from combined_detections.pkl29000\n",
      "Loading detections from combined_detections.pkl30000\n",
      "Loading detections from combined_detections.pkl31000\n",
      "Loading detections from combined_detections.pkl32000\n",
      "Loading detections from combined_detections.pkl33000\n",
      "Loading detections from combined_detections.pkl34000\n",
      "Loading detections from combined_detections.pkl35000\n",
      "Loading detections from combined_detections.pkl36000\n",
      "Loading detections from combined_detections.pkl37000\n",
      "Loading detections from combined_detections.pkl38000\n",
      "Loading detections from combined_detections.pkl39000\n",
      "Loading detections from combined_detections.pkl40000\n",
      "Loading detections from combined_detections.pkl41000\n",
      "Loading detections from combined_detections.pkl42000\n",
      "Loading detections from combined_detections.pkl43000\n",
      "Loading detections from combined_detections.pkl44000\n",
      "Loading detections from combined_detections.pkl45000\n",
      "Loading detections from combined_detections.pkl46000\n",
      "Loading detections from combined_detections.pkl47000\n",
      "Loading detections from combined_detections.pkl48000\n",
      "Loading detections from combined_detections.pkl49000\n",
      "Loading detections from combined_detections.pkl50000\n",
      "Loading detections from combined_detections.pkl51000\n",
      "Loading detections from combined_detections.pkl52000\n",
      "Loading detections from combined_detections.pkl53000\n",
      "Loading detections from combined_detections.pkl54000\n",
      "Loading detections from combined_detections.pkl55000\n",
      "Loading detections from combined_detections.pkl56000\n",
      "Loading detections from combined_detections.pkl57000\n",
      "Loading detections from combined_detections.pkl58000\n",
      "Loading detections from combined_detections.pkl58682\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T22:25:45.474349400Z",
     "start_time": "2025-06-02T18:01:46.232313Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e0b63d0354b93cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6596fbf7e2e7413b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
